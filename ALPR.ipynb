{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (2.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alvino angelo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((64, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset_path = 'test'  # Replace with your dataset path\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataLoader is returning batches\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALPRModel(nn.Module):\n",
    "    def __init__(self, num_chars=7):\n",
    "        super(ALPRModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_chars * 36)  # 7 characters, each with 36 possible classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 7, 36)  # Reshape output to (batch_size, num_chars, num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPRModel(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=252, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ALPRModel(num_chars=7).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(outputs[:, j, :], \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move images and labels to the GPU if available\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss for each character in the sequence\n",
    "            loss = 0\n",
    "            for j in range(7):\n",
    "                loss += criterion(outputs[:, j, :], labels[:, j])\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate running loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alvino Angelo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move and/or cast the parameters and buffers.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    .. function:: to(device=None, dtype=None, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    .. function:: to(dtype, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \n\u001b[0;32m   1063\u001b[0m \u001b[38;5;124;03m    .. function:: to(tensor, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \n\u001b[0;32m   1066\u001b[0m \u001b[38;5;124;03m    .. function:: to(memory_format=torch.channels_last)\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    floating point or complex :attr:`dtype`\\ s. In addition, this method will\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    only cast the floating point or complex parameters and buffers to :attr:`dtype`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    (if given). The integral parameters and buffers will be moved\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    :attr:`device`, if that is given, but with dtypes unchanged. When\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03m    :attr:`non_blocking` is set, it tries to convert/move asynchronously\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m    with respect to the host if possible, e.g., moving CPU Tensors with\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03m    pinned memory to CUDA devices.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \n\u001b[0;32m   1078\u001b[0m \u001b[38;5;124;03m    See below for examples.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \n\u001b[0;32m   1080\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;124;03m        This method modifies the module in-place.\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;124;03m        device (:class:`torch.device`): the desired device of the parameters\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03m            and buffers in this module\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;124;03m        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;124;03m            the parameters and buffers in this module\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m        tensor (torch.Tensor): Tensor whose dtype and device are the desired\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m            dtype and device for all parameters and buffers in this module\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m        memory_format (:class:`torch.memory_format`): the desired memory\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m            format for 4D parameters and buffers in this module (keyword\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m            only argument)\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \n\u001b[0;32m   1094\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \n\u001b[0;32m   1099\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]])\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03m        >>> linear.to(torch.double)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[1;32m-> 1110\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]], dtype=torch.float64)\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m        >>> gpu1 = torch.device(\"cuda:1\")\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;124;03m        >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;124;03m        >>> cpu = torch.device(\"cpu\")\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;124;03m        >>> linear.to(cpu)\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16)\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.3741+0.j,  0.2382+0.j],\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;124;03m                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;124;03m        >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;124;03m        tensor([[0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \n\u001b[0;32m   1137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mALPRModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alvino Angelo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move and/or cast the parameters and buffers.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    .. function:: to(device=None, dtype=None, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    .. function:: to(dtype, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \n\u001b[0;32m   1063\u001b[0m \u001b[38;5;124;03m    .. function:: to(tensor, non_blocking=False)\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \n\u001b[0;32m   1066\u001b[0m \u001b[38;5;124;03m    .. function:: to(memory_format=torch.channels_last)\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    floating point or complex :attr:`dtype`\\ s. In addition, this method will\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    only cast the floating point or complex parameters and buffers to :attr:`dtype`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    (if given). The integral parameters and buffers will be moved\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    :attr:`device`, if that is given, but with dtypes unchanged. When\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03m    :attr:`non_blocking` is set, it tries to convert/move asynchronously\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m    with respect to the host if possible, e.g., moving CPU Tensors with\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03m    pinned memory to CUDA devices.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \n\u001b[0;32m   1078\u001b[0m \u001b[38;5;124;03m    See below for examples.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \n\u001b[0;32m   1080\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;124;03m        This method modifies the module in-place.\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;124;03m        device (:class:`torch.device`): the desired device of the parameters\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03m            and buffers in this module\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;124;03m        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;124;03m            the parameters and buffers in this module\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m        tensor (torch.Tensor): Tensor whose dtype and device are the desired\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m            dtype and device for all parameters and buffers in this module\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m        memory_format (:class:`torch.memory_format`): the desired memory\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m            format for 4D parameters and buffers in this module (keyword\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m            only argument)\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \n\u001b[0;32m   1094\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \n\u001b[0;32m   1099\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]])\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03m        >>> linear.to(torch.double)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[1;32m-> 1110\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]], dtype=torch.float64)\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m        >>> gpu1 = torch.device(\"cuda:1\")\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;124;03m        >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;124;03m        >>> cpu = torch.device(\"cpu\")\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;124;03m        >>> linear.to(cpu)\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16)\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.3741+0.j,  0.2382+0.j],\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;124;03m                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;124;03m        >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;124;03m        tensor([[0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \n\u001b[0;32m   1137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Alvino Angelo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    443\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    444\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    446\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m, _pair(\u001b[38;5;241m0\u001b[39m), groups, bias, padding_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Alvino Angelo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m kernel_size_ \u001b[38;5;241m=\u001b[39m _pair(kernel_size)\n\u001b[0;32m    442\u001b[0m stride_ \u001b[38;5;241m=\u001b[39m _pair(stride)\n\u001b[1;32m--> 443\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    444\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    446\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m, _pair(\u001b[38;5;241m0\u001b[39m), groups, bias, padding_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: EFDM1HF\n"
     ]
    }
   ],
   "source": [
    "def predict(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "        outputs = model(image)\n",
    "        \n",
    "        predicted_chars = []\n",
    "        for i in range(7):\n",
    "            _, predicted = torch.max(outputs[0, i, :], 0)\n",
    "            predicted_chars.append(predicted.item())\n",
    "        \n",
    "        # Convert the predicted indices to corresponding characters (digits or letters)\n",
    "        alphabet = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        predicted_label = ''.join([alphabet[i] for i in predicted_chars])\n",
    "        return predicted_label\n",
    "\n",
    "# Predict the label from a new image\n",
    "image_path = 'A4721UT.png'\n",
    "image = Image.open(image_path)\n",
    "predicted_label = predict(model, image)\n",
    "print(f'Predicted Label: {predicted_label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
